# Export and Deploy Automodel LLMs

Automodel LLMs are a family of large language models designed for efficient inference and deployment in production environments. This section provides an overview of how to export, deploy, and serve Automodel LLMs using NVIDIA's optimized workflows.

You will find step-by-step guides for deploying Automodel LLMs with both Triton Inference Server and Ray Serve, as well as instructions for exporting models and integrating them into your own applications. Whether you are looking to serve models at scale or experiment with local deployments, the following pages will help you get started quickly.


```{toctree}
:maxdepth: 4
:titlesonly:
:hidden:

Deploy with Triton  <automodel-in-framework.md>
Deploy with Ray Serve  <automodel-ray.md>
Export and Deploy <optimized/index.md>
```