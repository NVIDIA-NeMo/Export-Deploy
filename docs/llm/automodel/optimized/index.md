# Export and Deploy NeMo Automodel LLMs

NeMo Export-Deploy library offers scripts and APIs to export models to two inference optimized libraries, TensorRT-LLM and vLLM, and to deploy the exported model with the NVIDIA Triton Inference Server and Ray Serve. 

```{toctree}
:maxdepth: 4
:titlesonly:
:hidden:

Deploy TensorRT-LLM with Triton <automodel-trtllm.md>
```