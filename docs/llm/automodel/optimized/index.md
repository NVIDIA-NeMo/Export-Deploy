# Deploy NeMo Models by Exporting to Inference Optimized Libraries

NeMo Framework offers scripts and APIs to export models to two inference optimized libraries, TensorRT-LLM and vLLM, and to deploy the exported model with the NVIDIA Triton Inference Server. Please check the table below to see which models are supported.

```{toctree}
:maxdepth: 4
:titlesonly:
:hidden:

Deploy TensorRT-LLM with Triton <automodel-trtllm.md>
```