# Copyright (c) 2025, NVIDIA CORPORATION. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

FROM nvcr.io/nvidia/pytorch:25.06-py3
WORKDIR /workspace
COPY . .
ARG INFERENCE_FRAMEWORK
ENV PATH="/root/.local/bin:$PATH"
ENV UV_PROJECT_ENVIRONMENT=/opt/venv
ENV PATH="$UV_PROJECT_ENVIRONMENT/bin:$PATH"
ENV UV_LINK_MODE=copy
ENV LD_LIBRARY_PATH=/opt/hpcx/ucx/lib:/usr/local/cuda/lib64:$LD_LIBRARY_PATH
ENV VLLM_USE_STANDALONE_COMPILE=0

RUN --mount=type=cache,target=/var/cache/apt,sharing=locked \
    --mount=type=cache,target=/var/lib/apt,sharing=locked \
    # --mount=type=cache,target=/root/.cache/uv \
    --mount=type=secret,id=GH_TOKEN bash -ex <<"EOF"
    export PAT=$(cat /run/secrets/GH_TOKEN)
    bash docker/common/install.sh --base-image pytorch --use-uv --inference-framework $INFERENCE_FRAMEWORK
    uv cache prune --ci
EOF

RUN --mount=type=bind,source=external/patches/triton-lang_triton_6570_lazy_init.patch,target=/opt/triton-lang_triton_6570_lazy_init.patch \
    cd /usr/local/lib/python3.12/dist-packages && \
    patch -p1 $(python -c "import triton; print(triton.__path__[0])")/runtime/autotuner.py /opt/triton-lang_triton_6570_lazy_init.patch